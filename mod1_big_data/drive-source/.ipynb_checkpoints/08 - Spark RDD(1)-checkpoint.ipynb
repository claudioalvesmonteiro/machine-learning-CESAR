{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para que o Jupyter consiga carregar o Spark corretamente no notebook\n",
    "import findspark\n",
    "findspark.init('/usr/local/Cellar/apache-spark/2.4.1/libexec')  # Caminho default da instalação no MAC OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para que os executors tenham mais memória e não falhem por falta de recursos\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--executor-memory 1G pyspark-shell'\n",
    "\n",
    "# A partir daqui é código Spark que normalmente é executado com um comando similar ao comando abaixo:\n",
    "# spark-submit --executor-memory 1G nome_do_script.py\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding SparkContext\n",
    "* SparkContext is an entry point into the world of Spark\n",
    "* An entry point is a way of connecting to Spark cluster\n",
    "* An entry point is like a key to the house\n",
    "* PySpark has a default SparkContext called sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version: To retrieve SparkContext version\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python Version: To retrieve Python version of SparkContext\n",
    "sc.pythonVer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Master: URL of the cluster or “local” string to run in local mode of SparkContext\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando dados no PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download do arquivo para local. Assim não precisaremos setar as credenciais do S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar os arquivos contidos no bucket\n",
    "from s3fs import S3FileSystem\n",
    "s3 = S3FileSystem(anon=True)\n",
    "\n",
    "details = s3.ls('cesarschool-data-samples/')\n",
    "print(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-05-18 06:52:36--  https://s3.us-east-2.amazonaws.com/cesarschool-data-samples/Shakespeare.txt\n",
      "Resolvendo s3.us-east-2.amazonaws.com (s3.us-east-2.amazonaws.com)... 52.219.96.122\n",
      "Conectando-se a s3.us-east-2.amazonaws.com (s3.us-east-2.amazonaws.com)|52.219.96.122|:443... conectado.\n",
      "A requisic~ao HTTP foi enviada, aguardando resposta... 200 OK\n",
      "Tamanho: 5784591 (5.5M) [text/plain]\n",
      "Salvando em: \"Shakespeare.txt\"\n",
      "\n",
      "Shakespeare.txt     100%[===================>]   5.52M  3.32MB/s    em 1.7s    \n",
      "\n",
      "2019-05-18 06:52:38 (3.32 MB/s) - \"Shakespeare.txt\" salvo [5784591/5784591]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!wget --no-check-certificate --no-proxy 'https://s3.us-east-2.amazonaws.com/cesarschool-data-samples/ml-100k/u.data'\n",
    "#!wget --no-check-certificate --no-proxy 'https://s3.us-east-2.amazonaws.com/cesarschool-data-samples/ml-100k/u.item'\n",
    "!wget --no-check-certificate --no-proxy 'https://s3.us-east-2.amazonaws.com/cesarschool-data-samples/Shakespeare.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = sc.textFile(\"u.data\")\n",
    "movies = sc.textFile(\"u.item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['196\\t242\\t3\\t881250949',\n",
       " '186\\t302\\t3\\t891717742',\n",
       " '22\\t377\\t1\\t878887116',\n",
       " '244\\t51\\t2\\t880606923',\n",
       " '166\\t346\\t1\\t886397596']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# SparkContext's parallelize() method\n",
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "helloRDD = sc.parallelize(\"Hello world\")\n",
    "\n",
    "print(type(helloRDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# SparkContext's textFile() method\n",
    "rdd2 = sc.textFile(\"Shakespeare.txt\", minPartitions = 6)\n",
    "print(type(rdd2))\n",
    "print(rdd2.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map()\n",
    "RDD = sc.parallelize([1,2,3,4])\n",
    "RDD_map = RDD.map(lambda x: x * x)\n",
    "RDD_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6110 usuários colocaram a nota 1\n",
      "11370 usuários colocaram a nota 2\n",
      "27145 usuários colocaram a nota 3\n",
      "34174 usuários colocaram a nota 4\n",
      "21201 usuários colocaram a nota 5\n"
     ]
    }
   ],
   "source": [
    "#Ex: Contagem por valor\n",
    "\n",
    "import collections\n",
    "\n",
    "rating_values = ratings.map(lambda x: x.split()[2])\n",
    "result = rating_values.countByValue()\n",
    "\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for rating, number_of_ratings in sortedResults.items():\n",
    "    print(\"{} usuários colocaram a nota {}\".format(number_of_ratings, rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'3': 27145, '1': 6110, '2': 11370, '4': 34174, '5': 21201})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rating_values.take(10)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter()\n",
    "RDD = sc.parallelize([1,2,3,4])\n",
    "RDD_filter = RDD.filter(lambda x: x > 2)\n",
    "RDD_filter.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A pior nota do filme 50 foi 1\n"
     ]
    }
   ],
   "source": [
    "# Ex: Filtro\n",
    "\n",
    "def parse_movie_and_rating(line):\n",
    "    fields = line.split()\n",
    "    movie_field = fields[1]\n",
    "    rating_field = fields[2]\n",
    "    return (movie_field, rating_field)\n",
    "\n",
    "ratings_by_movie = ratings.map(parse_movie_and_rating)\n",
    "star_wars_ratings = ratings_by_movie.filter(lambda x: \"50\" == x[0])\n",
    "min_start_wars_rating = star_wars_ratings.reduceByKey(lambda x, y: min(x, y))\n",
    "results = min_start_wars_rating.collect()\n",
    "\n",
    "for movie, rating in results:\n",
    "    print(\"A pior nota do filme {} foi {}\".format(movie, rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap()\n",
    "# flatMap() transformation returns multiple values for each element in the original RDD\n",
    "RDD = sc.parallelize([\"hello world\", \"how are you\"])\n",
    "RDD_flatmap = RDD.flatMap(lambda x: x.split(\" \"))\n",
    "RDD_flatmap.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations on pair RDDs\n",
    "* All regular transformations work on pair RDD\n",
    "* Have to pass functions that operate on key value pairs rather than on individual elements\n",
    "* Examples of paired RDD Transformations\n",
    "    * `reduceByKey(func)`: Combine values with the same key\n",
    "    * `groupByKey()`: Group values with the same key\n",
    "    * `sortByKey()`: Return an RDD sorted by the key\n",
    "    * `join()`: Join two pair RDDs based on their key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex: Pares RDD - Tuplas\n",
    "\n",
    "def parse_ratings_as_key(line):\n",
    "    fields = line.split()\n",
    "    rating_field = int(fields[2])\n",
    "    return (rating_field, 1)\n",
    "\n",
    "ratings_count = ratings.map(parse_ratings_as_key)\n",
    "ratings_sum = ratings_count.reduceByKey(lambda x, y: x + y)\n",
    "results = ratings_sum.collect()\n",
    "for rating, number_of_ratings in results:\n",
    "    print(\"{} usuários colocaram a nota {}\".format(number_of_ratings, rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo: Palavas mais comuns em um texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de palavras em splitRDD: 961054\n"
     ]
    }
   ],
   "source": [
    "# Cria um RDD a partir do caminho do arquivo\n",
    "baseRDD = sc.textFile('Shakespeare.txt')\n",
    "\n",
    "# Quebra as linhas do baseRDD em palavras\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
    "\n",
    "# Conta o número total de palavras\n",
    "print(\"Número total de palavras em splitRDD:\", splitRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project', 'Gutenberg’s', 'The', 'Complete', 'Works']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseRDD.take(5)\n",
    "#splitRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words são as palavras desinteressantes para a análise, como conectivos, pronomes, etc\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', \n",
    "              'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', \n",
    "              'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', \n",
    "              'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', \n",
    "              'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', \n",
    "              'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', \n",
    "              'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', \n",
    "              'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', \n",
    "              'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', \n",
    "              'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', \n",
    "              'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', \n",
    "              'just', 'don', 'should', 'now']\n",
    "\n",
    "# Converte as palavras para minúsculo e remove as stop words\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# Cria um par RDD (tupla) com a palavra e 1\n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Conta o número de ocorrências de cada palavra\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Project', 85),\n",
       " ('Gutenberg’s', 2),\n",
       " ('Shakespeare', 6),\n",
       " ('use', 288),\n",
       " ('anyone', 8),\n",
       " ('anywhere', 4),\n",
       " ('United', 15),\n",
       " ('States', 8),\n",
       " ('world', 376),\n",
       " ('restrictions', 2)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thou aparece 4518 vezes\n",
      "thy aparece 3919 vezes\n",
      "shall aparece 3248 vezes\n",
      "good aparece 2171 vezes\n",
      "would aparece 2132 vezes\n",
      "Enter aparece 1997 vezes\n",
      "thee aparece 1886 vezes\n",
      "hath aparece 1719 vezes\n",
      "like aparece 1642 vezes\n",
      "make aparece 1564 vezes\n"
     ]
    }
   ],
   "source": [
    "# Precisamos ordenar pela contagem\n",
    "\n",
    "# Inverte as chaves e valores\n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Ordena as novas chaves em ordem descendente\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "# Mostra as 10 palavras mais frequentes e suas freqências\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "    print(\"{} aparece {} vezes\". format(word[1], word[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "def parse_ratings(line):\n",
    "    fields = line.split()\n",
    "    return Row(user_id=int(fields[0]), \n",
    "               movie_id=int(fields[1]), \n",
    "               rating=int(fields[2]), \n",
    "               timestamp=int(fields[3]))\n",
    "\n",
    "def parse_movies(line):\n",
    "    fields = line.split(\"|\")\n",
    "    return Row(movie_id=int(fields[0]), \n",
    "               name=fields[1])\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "\n",
    "data = spark.sparkContext.textFile(\"u.data\")\n",
    "ratings = data.map(parse_ratings)\n",
    "ratings_df = spark.createDataFrame(ratings).cache()\n",
    "ratings_df.createOrReplaceTempView(\"ratings\")\n",
    "\n",
    "data = spark.sparkContext.textFile(\"u.item\")\n",
    "movies = data.map(parse_movies)\n",
    "movies_df = spark.createDataFrame(movies).cache()\n",
    "movies_df.createOrReplaceTempView(\"movies\")\n",
    "\n",
    "# AVISO: lembre de executar o comando spark.stop() no último bloco de código quando acabar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(movie_id=465, rating=5)\n",
      "Row(movie_id=1014, rating=5)\n",
      "Row(movie_id=222, rating=5)\n",
      "Row(movie_id=387, rating=5)\n",
      "Row(movie_id=95, rating=5)\n",
      "Row(movie_id=234, rating=5)\n",
      "Row(movie_id=603, rating=5)\n",
      "Row(movie_id=327, rating=5)\n",
      "Row(movie_id=201, rating=5)\n",
      "Row(movie_id=1137, rating=5)\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"SELECT movie_id, rating FROM ratings WHERE rating = 5 LIMIT 10\")\n",
    "\n",
    "for r in result.collect():\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|movie_id|rating|\n",
      "+--------+------+\n",
      "|     465|     5|\n",
      "|    1014|     5|\n",
      "|     222|     5|\n",
      "|     387|     5|\n",
      "|      95|     5|\n",
      "|     234|     5|\n",
      "|     603|     5|\n",
      "|     327|     5|\n",
      "|     201|     5|\n",
      "|    1137|     5|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
